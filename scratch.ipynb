{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import math\n",
    "from typing import Tuple\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(265, 51)\n",
      "(32620, 4)\n",
      "(265, 3)\n"
     ]
    }
   ],
   "source": [
    "patients = pd.read_csv(\"data/patients.csv\").fillna(0)\n",
    "encounters = pd.read_csv(\"data/admissions.csv\").iloc[:, 1:].fillna(0)\n",
    "lab_events = pd.read_csv(\"data/lab_events.csv\").iloc[:, [0, 1, 2, 3]].fillna(0)\n",
    "labels = pd.read_csv(\"data/labels.csv\").fillna(0)\n",
    "print(patients.shape)\n",
    "print(encounters.shape)\n",
    "print(lab_events.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>READMIT_ONE_WEEK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10325135</td>\n",
       "      <td>25246220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10328843</td>\n",
       "      <td>21438566</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10500141</td>\n",
       "      <td>24121499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10761013</td>\n",
       "      <td>27708487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10761013</td>\n",
       "      <td>27436739</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id  READMIT_ONE_WEEK\n",
       "0    10325135  25246220                 0\n",
       "1    10328843  21438566                 0\n",
       "2    10500141  24121499                 0\n",
       "3    10761013  27708487                 0\n",
       "4    10761013  27436739                 1"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lab_types = len(lab_events[\"itemid\"].unique()) # 317 unique lab categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientEncounter(Dataset):\n",
    "    '''\n",
    "    PatientEncounter Dataset class\n",
    "    '''\n",
    "    def __init__(self, patients, encounters, lab_events, labels):\n",
    "        super().__init__()\n",
    "        self.patients = patients\n",
    "        self.encounters = encounters\n",
    "        self.lab_events = lab_events\n",
    "        self.labels = labels\n",
    "        self.patient_ids = encounters.subject_id\n",
    "        self.encounter_ids = encounters.hadm_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        patient_id = self.patient_ids[index]\n",
    "        encounter_id = self.encounter_ids[index]\n",
    "        \n",
    "        # Load data for the given patient-encounter\n",
    "        data_patient = torch.from_numpy(self.patients.loc[self.patients.subject_id == patient_id].iloc[:, 1:].astype(float).values).to(device)\n",
    "        data_encounter = torch.from_numpy(self.encounters.loc[(self.encounters.subject_id == patient_id) & (self.encounters.hadm_id == encounter_id)].iloc[:, 2:].astype(float).values).to(device)\n",
    "        data_lab_events = torch.from_numpy(self.lab_events.loc[(self.lab_events.subject_id == patient_id) & (self.lab_events.hadm_id == encounter_id)].iloc[:, 2:].astype(float).values).to(device)\n",
    "        X = [data_patient, data_encounter, data_lab_events]\n",
    "        y = torch.from_numpy(self.labels.loc[(self.labels.subject_id == patient_id) & (self.labels.hadm_id == encounter_id)].READMIT_ONE_WEEK.values).to(device)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "class Patient(Dataset):\n",
    "    '''\n",
    "    Patient Dataset class\n",
    "    '''\n",
    "    def __init__(self, patients, labels):\n",
    "        super().__init__()\n",
    "        self.patients = patients\n",
    "        self.labels = labels\n",
    "        self.patient_ids = patients.subject_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        patient_id = self.patient_ids[index]\n",
    "        \n",
    "        # Load data for the given patient\n",
    "        data_patient = torch.from_numpy(self.patients.loc[self.patients.subject_id == patient_id].values).to(device)\n",
    "        X = [data_patient]\n",
    "        y = torch.from_numpy(self.labels.loc[self.labels.subject_id == patient_id].READMIT_ONE_WEEK.values).to(device)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "# Define a custom collate function to pad tensors in X to the same size\n",
    "def collate_fn(batch):\n",
    "    X, y = zip(*batch)\n",
    "    X_patient = [x[0] for x in X]\n",
    "    X_encounter = [x[1] for x in X]\n",
    "    X_lab_events = [x[2] for x in X]\n",
    "    X_lab_events_padded = pad_sequence(X_lab_events, batch_first=True, padding_value=0)\n",
    "    X_lab_events_padded = [x.unsqueeze(0) for x in X_lab_events_padded]\n",
    "    y = torch.cat(y, dim=0)\n",
    "    return X_patient, X_encounter, X_lab_events_padded, y\n",
    "\n",
    "train_data = PatientEncounter(patients, encounters, lab_events, labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1]),\n",
       " torch.Size([1, 49]),\n",
       " torch.Size([1, 1044, 2]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_x = next(iter(train_dataloader))\n",
    "example_x[0][0].shape, example_x[1][0].shape, example_x[2][0].shape, example_x[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5518],\n",
       "        [0.5517],\n",
       "        [0.5523],\n",
       "        [0.5512],\n",
       "        [0.5519],\n",
       "        [0.5539],\n",
       "        [0.5542],\n",
       "        [0.5541],\n",
       "        [0.5534],\n",
       "        [0.5520],\n",
       "        [0.5528],\n",
       "        [0.5534],\n",
       "        [0.5531],\n",
       "        [0.5537],\n",
       "        [0.5534],\n",
       "        [0.5533],\n",
       "        [0.5498],\n",
       "        [0.5499],\n",
       "        [0.5499],\n",
       "        [0.5518],\n",
       "        [0.5498],\n",
       "        [0.5496],\n",
       "        [0.5531],\n",
       "        [0.5502],\n",
       "        [0.5507],\n",
       "        [0.5505],\n",
       "        [0.5520],\n",
       "        [0.5499],\n",
       "        [0.5530],\n",
       "        [0.5501],\n",
       "        [0.5537],\n",
       "        [0.5538],\n",
       "        [0.5540],\n",
       "        [0.5541],\n",
       "        [0.5538],\n",
       "        [0.5528],\n",
       "        [0.5536],\n",
       "        [0.5536],\n",
       "        [0.5521],\n",
       "        [0.5524],\n",
       "        [0.5534],\n",
       "        [0.5521],\n",
       "        [0.5503],\n",
       "        [0.5540],\n",
       "        [0.5531],\n",
       "        [0.5536],\n",
       "        [0.5536],\n",
       "        [0.5536],\n",
       "        [0.5537],\n",
       "        [0.5537],\n",
       "        [0.5543],\n",
       "        [0.5521],\n",
       "        [0.5542],\n",
       "        [0.5541],\n",
       "        [0.5535],\n",
       "        [0.5537],\n",
       "        [0.5535],\n",
       "        [0.5532],\n",
       "        [0.5546],\n",
       "        [0.5540],\n",
       "        [0.5533],\n",
       "        [0.5538],\n",
       "        [0.5544],\n",
       "        [0.5543]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LabModule(nn.Module):\n",
    "    '''\n",
    "    Lab specific module, which takes in a batch of lab events and outputs a batch of lab embeddings.\n",
    "    '''\n",
    "    def __init__(self, num_lab_types):\n",
    "        super().__init__()\n",
    "        self.lab_type = nn.Linear(in_features=1, out_features=5) # 5-dim lab type embedding\n",
    "        self.lab_value = nn.Linear(in_features=1, out_features=5) # 5-dim lab value embedding\n",
    "        self.layer_out = nn.Linear(in_features=10, out_features=5) # 5-dim lab output embedding\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.cat(x, dim=0) # dim = (batch_size, max_seq_len, 2)\n",
    "        x_type = x[:, :, 0].unsqueeze(2)\n",
    "        x_value = x[:, :, 1].unsqueeze(2)\n",
    "        out = torch.cat((self.lab_type(x_type), self.lab_value(x_value)), dim=2)\n",
    "        out = self.layer_out(out) # dim = (batch_size, max_seq_len, 5)\n",
    "        return(out)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1) # dim = (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)) # dim = (d_model/2)\n",
    "        pe = torch.zeros(max_len, 1, d_model) # dim = (max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class PatientEncounterModel(nn.Module):\n",
    "    '''\n",
    "    Patient encounter DL model structure.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int, # embedding dimension\n",
    "        nhead: int, # number of heads in multi-head attention\n",
    "        d_hid: int, # dimension of feedforward network model\n",
    "        nlayers: int, # number of encoder layers\n",
    "        dropout: float, # dropout probability\n",
    "        embed_event_dim: int, # dimension of event embedding\n",
    "        num_static_features: int, # number of static features\n",
    "        num_transformer_layers: 2, # number of transformer layers\n",
    "        num_mlp_layers: int=2, # number of MLP layers\n",
    "        num_mlp_features: int=11, # number of MLP features\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Linear(embed_event_dim, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, embed_event_dim)\n",
    "        self.lab_module = LabModule(num_lab_types=num_lab_types)\n",
    "        self.layer_out = nn.Linear(in_features=num_static_features+d_model, out_features=10)\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.fc2 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.fc3 = nn.Linear(in_features=10, out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.lab_module(x[2]) # only lab-specific module for now\n",
    " \n",
    "        # 4: feed modality-specific outputs into transformer architecture\n",
    "        out = self.encoder(out) * math.sqrt(self.d_model)\n",
    "        out = self.pos_encoder(out)\n",
    "        out = self.transformer_encoder(out)\n",
    "        out = torch.mean(out, dim=1, keepdim=True)\n",
    "\n",
    "        # 5: concatenate outputs from transformer structure with static features\n",
    "        out = torch.stack([out], dim=0).squeeze(0).squeeze(1)\n",
    "        x_patient = torch.stack(x[0], dim=0).squeeze(2)\n",
    "        x_encounter = torch.stack(x[1], dim=0).squeeze(1)\n",
    "        out = torch.cat([x_patient, x_encounter, out], dim=1)\n",
    "\n",
    "        # 6: feed concatenated output into MLP architecture\n",
    "        out = self.layer_out(out)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.relu(self.fc3(out))\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return(out)\n",
    "\n",
    "model = PatientEncounterModel(\n",
    "    num_static_features=50, # number of static features\n",
    "    num_transformer_layers=2, # number of transformer layers\n",
    "    num_mlp_layers=2, # number of MLP layers\n",
    "    num_mlp_features=11, # number of MLP features\n",
    "    d_model=100, # embedding dimension\n",
    "    nhead=2, # number of heads in multi-head attention\n",
    "    d_hid=5, # dimension of feedforward network model\n",
    "    nlayers=2, # number of encoder layers\n",
    "    dropout=0.1, # dropout probability\n",
    "    embed_event_dim=5 # dimension of event embedding\n",
    ").to(device).double()\n",
    "\n",
    "x = next(iter(train_dataloader))\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [304], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      8\u001b[0m         \u001b[39m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m         inputs, labels \u001b[39m=\u001b[39m data\n\u001b[1;32m     10\u001b[0m         labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m         inputs \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m inputs]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train model\n",
    "for epoch in range(1, 10):\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        labels = labels.to(device)\n",
    "        inputs = [x.to(device) for x in inputs]\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
